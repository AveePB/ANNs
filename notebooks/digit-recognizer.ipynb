{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6327652b",
   "metadata": {},
   "source": [
    "# Digit Recognizer\n",
    "**MNIS** (\"*Modified National Institute of Standards and Technology*\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms.\n",
    "\n",
    "### Image Description\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n",
    "\n",
    "### Dataset Info\n",
    "The training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c000fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53fba00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91f5e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMNISTDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file, transform=None, is_test=False):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data_frame.iloc[index]\n",
    "\n",
    "        if self.is_test:\n",
    "            image = item.values.reshape(28, 28).astype(np.uint8)\n",
    "            label = None\n",
    "        else:\n",
    "            image = item[1:].values.reshape(28, 28).astype(np.uint8)\n",
    "            label = item.iloc[0]\n",
    "\n",
    "        image = transforms.ToPILImage()(image)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.is_test:\n",
    "            return image\n",
    "        else:\n",
    "            return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20772ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df595adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomMNISTDataset('./../datasets/digit-recognizer-train.csv', transform=transform, is_test=False)\n",
    "test_dataset = CustomMNISTDataset('./../datasets/digit-recognizer-test.csv', transform=transform, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a89d8d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 42000 , Test Size: 28000\n"
     ]
    }
   ],
   "source": [
    "print('Train Size:', len(train_dataset), ', Test Size:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c314a9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -0.8588, -0.7647,  0.0745,  0.0745,\n",
       "            0.5059, -0.3255, -0.4353, -0.9922, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -0.8980, -0.3255,  0.9608,  0.9922,  0.9922,  0.9922,\n",
       "            0.9922,  0.7020,  0.9294,  0.1843, -0.7490, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -0.8745,  0.4039,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,\n",
       "            0.9922,  0.9922,  0.9922,  0.9922,  0.8118, -0.5765, -0.8824,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -0.4353,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,\n",
       "            0.9922,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922, -0.1843,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5216,\n",
       "            0.4980,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922, -0.1451,\n",
       "           -0.3490,  0.5608,  0.9922,  0.9922,  0.9922,  0.9922,  0.9059,\n",
       "           -0.3333, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.3490,\n",
       "            0.9922,  0.9922,  0.9922,  0.5843,  0.1529,  0.1529, -0.6471,\n",
       "           -1.0000, -0.9137, -0.7725,  0.5686,  0.9922,  0.9922,  0.9922,\n",
       "            0.3412, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9922,  0.3647,\n",
       "            0.9922,  0.9922, -0.3020, -0.4745, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000,  0.0039,  0.9765,  0.9922,  0.9922,\n",
       "            0.6627, -0.4039, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6314,  0.9922,\n",
       "            0.9922,  0.9922, -0.7725, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -0.3490,  0.9922,  0.9922,\n",
       "            0.9922,  0.2000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3725,  0.9922,\n",
       "            0.9922,  0.8824, -0.8118, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -0.8039,  0.8824,  0.9922,\n",
       "            0.9922,  0.2000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4980,  0.9922,\n",
       "            0.9922,  0.4588, -0.9451, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.3020,  0.9922,\n",
       "            0.9922,  0.7569, -0.9059, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -0.8902,  0.8196,  0.9922,\n",
       "            0.9922,  0.9922, -0.7725, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4118,  0.9922,\n",
       "            0.9922,  0.9922, -0.8667, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -0.8588,  0.9922,  0.9922,\n",
       "            0.9922,  0.9922, -0.7725, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6235,  0.9922,\n",
       "            0.9922,  0.9922, -0.8667, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -0.9843,  0.2784,  0.9922,\n",
       "            0.9922,  0.9922, -0.7725, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6235,  0.9922,\n",
       "            0.9922,  0.9922, -0.8667, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.2627,  0.9922,\n",
       "            0.9922,  0.9922,  0.5686, -0.9059, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -0.8745,  0.6392,  0.9922,\n",
       "            0.9922,  0.1765, -0.9922, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8824,  0.6157,\n",
       "            0.9922,  0.9922,  0.9922,  0.5843, -0.4824, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -0.8353,  0.2627,  0.9922,  0.9922,\n",
       "            0.9216, -0.7569, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5294,\n",
       "            0.6627,  0.9922,  0.9922,  0.9922,  0.5216, -0.6235, -0.6235,\n",
       "           -0.7333, -0.6784, -0.6235,  0.6392,  0.9922,  0.9922,  0.9922,\n",
       "            0.3412, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -0.3255,  0.9059,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,\n",
       "            0.8275,  0.9059,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,\n",
       "           -0.3255, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -0.1059,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,\n",
       "            0.9922,  0.9922,  0.9922,  0.9922,  0.9922,  0.8745, -0.3255,\n",
       "           -0.9137, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -0.8980,  0.4275,  0.9922,  0.9922,  0.9922,  0.9922,\n",
       "            0.9922,  0.9922,  0.9922,  0.9922,  0.9059, -0.4510, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -0.9373, -0.4039,  0.1451,  0.9922,  1.0000,\n",
       "            0.9922,  1.0000,  0.1451, -0.8510, -0.8824, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]]),\n",
       " np.int64(0))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b7ad7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aad9da73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size: torch.Size([1, 28, 28])\n",
      "Input Size: (28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for example_data, example_labels in train_loader:\n",
    "    example_image = example_data[0]\n",
    "    print('Input Size:', example_image.size())\n",
    "\n",
    "    example_image_numpy = example_image.permute(1, 2, 0).numpy()\n",
    "    print('Input Size:', example_image_numpy.shape)\n",
    "\n",
    "    plt.imshow(example_image_numpy.squeeze(), cmap='gray')\n",
    "    plt.title(f'Label: {example_labels[0]}')\n",
    "    plt.show()\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45c1f9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitCNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 7 * 7, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 20)\n",
    "        self.fc3 = nn.Linear(20, 10)\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            self.conv1,\n",
    "            self.relu,\n",
    "            self.pool,\n",
    "\n",
    "            self.conv2,\n",
    "            self.relu,\n",
    "            \n",
    "            self.conv3,\n",
    "            self.pool,\n",
    "        )\n",
    "\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            self.fc1,\n",
    "            self.relu,\n",
    "            self.fc2,\n",
    "            self.fc3\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "\n",
    "        # Update input form\n",
    "        x_size = x.size(1) * x.size(2) * x.size(3)\n",
    "        x = x.view(-1, x_size)\n",
    "\n",
    "        return self.linear_layers(x)\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8261c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DigitCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc305b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 100] Loss: 2.301\n",
      "[1, 200] Loss: 2.287\n",
      "[1, 300] Loss: 2.260\n",
      "[1, 400] Loss: 2.182\n",
      "[1, 500] Loss: 1.771\n",
      "[1, 600] Loss: 0.924\n",
      "[2, 100] Loss: 0.894\n",
      "[2, 200] Loss: 0.455\n",
      "[2, 300] Loss: 0.396\n",
      "[2, 400] Loss: 0.377\n",
      "[2, 500] Loss: 0.335\n",
      "[2, 600] Loss: 0.338\n",
      "[3, 100] Loss: 0.459\n",
      "[3, 200] Loss: 0.282\n",
      "[3, 300] Loss: 0.254\n",
      "[3, 400] Loss: 0.230\n",
      "[3, 500] Loss: 0.222\n",
      "[3, 600] Loss: 0.197\n",
      "[4, 100] Loss: 0.301\n",
      "[4, 200] Loss: 0.185\n",
      "[4, 300] Loss: 0.169\n",
      "[4, 400] Loss: 0.152\n",
      "[4, 500] Loss: 0.144\n",
      "[4, 600] Loss: 0.144\n",
      "[5, 100] Loss: 0.211\n",
      "[5, 200] Loss: 0.136\n",
      "[5, 300] Loss: 0.129\n",
      "[5, 400] Loss: 0.123\n",
      "[5, 500] Loss: 0.120\n",
      "[5, 600] Loss: 0.117\n",
      "[6, 100] Loss: 0.177\n",
      "[6, 200] Loss: 0.102\n",
      "[6, 300] Loss: 0.098\n",
      "[6, 400] Loss: 0.107\n",
      "[6, 500] Loss: 0.097\n",
      "[6, 600] Loss: 0.104\n",
      "[7, 100] Loss: 0.145\n",
      "[7, 200] Loss: 0.086\n",
      "[7, 300] Loss: 0.085\n",
      "[7, 400] Loss: 0.081\n",
      "[7, 500] Loss: 0.088\n",
      "[7, 600] Loss: 0.081\n",
      "[8, 100] Loss: 0.128\n",
      "[8, 200] Loss: 0.078\n",
      "[8, 300] Loss: 0.077\n",
      "[8, 400] Loss: 0.079\n",
      "[8, 500] Loss: 0.077\n",
      "[8, 600] Loss: 0.072\n",
      "[9, 100] Loss: 0.115\n",
      "[9, 200] Loss: 0.071\n",
      "[9, 300] Loss: 0.068\n",
      "[9, 400] Loss: 0.067\n",
      "[9, 500] Loss: 0.076\n",
      "[9, 600] Loss: 0.069\n",
      "[10, 100] Loss: 0.109\n",
      "[10, 200] Loss: 0.063\n",
      "[10, 300] Loss: 0.075\n",
      "[10, 400] Loss: 0.070\n",
      "[10, 500] Loss: 0.069\n",
      "[10, 600] Loss: 0.063\n",
      "[11, 100] Loss: 0.093\n",
      "[11, 200] Loss: 0.053\n",
      "[11, 300] Loss: 0.061\n",
      "[11, 400] Loss: 0.059\n",
      "[11, 500] Loss: 0.060\n",
      "[11, 600] Loss: 0.065\n",
      "[12, 100] Loss: 0.095\n",
      "[12, 200] Loss: 0.055\n",
      "[12, 300] Loss: 0.051\n",
      "[12, 400] Loss: 0.057\n",
      "[12, 500] Loss: 0.058\n",
      "[12, 600] Loss: 0.050\n",
      "[13, 100] Loss: 0.086\n",
      "[13, 200] Loss: 0.050\n",
      "[13, 300] Loss: 0.059\n",
      "[13, 400] Loss: 0.047\n",
      "[13, 500] Loss: 0.050\n",
      "[13, 600] Loss: 0.056\n",
      "[14, 100] Loss: 0.089\n",
      "[14, 200] Loss: 0.044\n",
      "[14, 300] Loss: 0.046\n",
      "[14, 400] Loss: 0.055\n",
      "[14, 500] Loss: 0.050\n",
      "[14, 600] Loss: 0.051\n",
      "[15, 100] Loss: 0.067\n",
      "[15, 200] Loss: 0.048\n",
      "[15, 300] Loss: 0.052\n",
      "[15, 400] Loss: 0.044\n",
      "[15, 500] Loss: 0.044\n",
      "[15, 600] Loss: 0.050\n",
      "[16, 100] Loss: 0.062\n",
      "[16, 200] Loss: 0.047\n",
      "[16, 300] Loss: 0.042\n",
      "[16, 400] Loss: 0.042\n",
      "[16, 500] Loss: 0.037\n",
      "[16, 600] Loss: 0.042\n",
      "[17, 100] Loss: 0.068\n",
      "[17, 200] Loss: 0.042\n",
      "[17, 300] Loss: 0.047\n",
      "[17, 400] Loss: 0.051\n",
      "[17, 500] Loss: 0.042\n",
      "[17, 600] Loss: 0.039\n",
      "[18, 100] Loss: 0.058\n",
      "[18, 200] Loss: 0.040\n",
      "[18, 300] Loss: 0.043\n",
      "[18, 400] Loss: 0.040\n",
      "[18, 500] Loss: 0.045\n",
      "[18, 600] Loss: 0.036\n",
      "[19, 100] Loss: 0.061\n",
      "[19, 200] Loss: 0.039\n",
      "[19, 300] Loss: 0.039\n",
      "[19, 400] Loss: 0.036\n",
      "[19, 500] Loss: 0.035\n",
      "[19, 600] Loss: 0.047\n",
      "[20, 100] Loss: 0.056\n",
      "[20, 200] Loss: 0.035\n",
      "[20, 300] Loss: 0.031\n",
      "[20, 400] Loss: 0.036\n",
      "[20, 500] Loss: 0.041\n",
      "[20, 600] Loss: 0.035\n",
      "[21, 100] Loss: 0.055\n",
      "[21, 200] Loss: 0.039\n",
      "[21, 300] Loss: 0.034\n",
      "[21, 400] Loss: 0.038\n",
      "[21, 500] Loss: 0.032\n",
      "[21, 600] Loss: 0.031\n",
      "[22, 100] Loss: 0.054\n",
      "[22, 200] Loss: 0.032\n",
      "[22, 300] Loss: 0.035\n",
      "[22, 400] Loss: 0.031\n",
      "[22, 500] Loss: 0.034\n",
      "[22, 600] Loss: 0.033\n",
      "[23, 100] Loss: 0.046\n",
      "[23, 200] Loss: 0.032\n",
      "[23, 300] Loss: 0.031\n",
      "[23, 400] Loss: 0.027\n",
      "[23, 500] Loss: 0.033\n",
      "[23, 600] Loss: 0.033\n",
      "[24, 100] Loss: 0.045\n",
      "[24, 200] Loss: 0.029\n",
      "[24, 300] Loss: 0.030\n",
      "[24, 400] Loss: 0.033\n",
      "[24, 500] Loss: 0.033\n",
      "[24, 600] Loss: 0.028\n",
      "[25, 100] Loss: 0.051\n",
      "[25, 200] Loss: 0.027\n",
      "[25, 300] Loss: 0.036\n",
      "[25, 400] Loss: 0.026\n",
      "[25, 500] Loss: 0.031\n",
      "[25, 600] Loss: 0.028\n",
      "[26, 100] Loss: 0.049\n",
      "[26, 200] Loss: 0.022\n",
      "[26, 300] Loss: 0.027\n",
      "[26, 400] Loss: 0.025\n",
      "[26, 500] Loss: 0.031\n",
      "[26, 600] Loss: 0.025\n",
      "[27, 100] Loss: 0.050\n",
      "[27, 200] Loss: 0.026\n",
      "[27, 300] Loss: 0.025\n",
      "[27, 400] Loss: 0.026\n",
      "[27, 500] Loss: 0.032\n",
      "[27, 600] Loss: 0.023\n",
      "[28, 100] Loss: 0.048\n",
      "[28, 200] Loss: 0.027\n",
      "[28, 300] Loss: 0.028\n",
      "[28, 400] Loss: 0.022\n",
      "[28, 500] Loss: 0.020\n",
      "[28, 600] Loss: 0.025\n",
      "[29, 100] Loss: 0.050\n",
      "[29, 200] Loss: 0.031\n",
      "[29, 300] Loss: 0.026\n",
      "[29, 400] Loss: 0.027\n",
      "[29, 500] Loss: 0.021\n",
      "[29, 600] Loss: 0.026\n",
      "[30, 100] Loss: 0.036\n",
      "[30, 200] Loss: 0.025\n",
      "[30, 300] Loss: 0.023\n",
      "[30, 400] Loss: 0.027\n",
      "[30, 500] Loss: 0.022\n",
      "[30, 600] Loss: 0.019\n",
      "[31, 100] Loss: 0.045\n",
      "[31, 200] Loss: 0.026\n",
      "[31, 300] Loss: 0.025\n",
      "[31, 400] Loss: 0.020\n",
      "[31, 500] Loss: 0.024\n",
      "[31, 600] Loss: 0.027\n",
      "[32, 100] Loss: 0.041\n",
      "[32, 200] Loss: 0.022\n",
      "[32, 300] Loss: 0.023\n",
      "[32, 400] Loss: 0.022\n",
      "[32, 500] Loss: 0.024\n",
      "[32, 600] Loss: 0.023\n",
      "[33, 100] Loss: 0.032\n",
      "[33, 200] Loss: 0.023\n",
      "[33, 300] Loss: 0.023\n",
      "[33, 400] Loss: 0.019\n",
      "[33, 500] Loss: 0.022\n",
      "[33, 600] Loss: 0.027\n",
      "[34, 100] Loss: 0.033\n",
      "[34, 200] Loss: 0.022\n",
      "[34, 300] Loss: 0.022\n",
      "[34, 400] Loss: 0.021\n",
      "[34, 500] Loss: 0.022\n",
      "[34, 600] Loss: 0.026\n",
      "[35, 100] Loss: 0.033\n",
      "[35, 200] Loss: 0.021\n",
      "[35, 300] Loss: 0.020\n",
      "[35, 400] Loss: 0.024\n",
      "[35, 500] Loss: 0.023\n",
      "[35, 600] Loss: 0.021\n",
      "[36, 100] Loss: 0.029\n",
      "[36, 200] Loss: 0.017\n",
      "[36, 300] Loss: 0.023\n",
      "[36, 400] Loss: 0.017\n",
      "[36, 500] Loss: 0.020\n",
      "[36, 600] Loss: 0.024\n",
      "[37, 100] Loss: 0.032\n",
      "[37, 200] Loss: 0.019\n",
      "[37, 300] Loss: 0.022\n",
      "[37, 400] Loss: 0.021\n",
      "[37, 500] Loss: 0.018\n",
      "[37, 600] Loss: 0.019\n",
      "[38, 100] Loss: 0.028\n",
      "[38, 200] Loss: 0.019\n",
      "[38, 300] Loss: 0.018\n",
      "[38, 400] Loss: 0.018\n",
      "[38, 500] Loss: 0.021\n",
      "[38, 600] Loss: 0.024\n",
      "[39, 100] Loss: 0.036\n",
      "[39, 200] Loss: 0.019\n",
      "[39, 300] Loss: 0.023\n",
      "[39, 400] Loss: 0.017\n",
      "[39, 500] Loss: 0.020\n",
      "[39, 600] Loss: 0.021\n",
      "[40, 100] Loss: 0.038\n",
      "[40, 200] Loss: 0.018\n",
      "[40, 300] Loss: 0.016\n",
      "[40, 400] Loss: 0.019\n",
      "[40, 500] Loss: 0.019\n",
      "[40, 600] Loss: 0.019\n",
      "[41, 100] Loss: 0.026\n",
      "[41, 200] Loss: 0.017\n",
      "[41, 300] Loss: 0.018\n",
      "[41, 400] Loss: 0.019\n",
      "[41, 500] Loss: 0.023\n",
      "[41, 600] Loss: 0.022\n",
      "[42, 100] Loss: 0.026\n",
      "[42, 200] Loss: 0.017\n",
      "[42, 300] Loss: 0.019\n",
      "[42, 400] Loss: 0.018\n",
      "[42, 500] Loss: 0.017\n",
      "[42, 600] Loss: 0.018\n",
      "[43, 100] Loss: 0.033\n",
      "[43, 200] Loss: 0.016\n",
      "[43, 300] Loss: 0.012\n",
      "[43, 400] Loss: 0.014\n",
      "[43, 500] Loss: 0.018\n",
      "[43, 600] Loss: 0.020\n",
      "[44, 100] Loss: 0.026\n",
      "[44, 200] Loss: 0.013\n",
      "[44, 300] Loss: 0.013\n",
      "[44, 400] Loss: 0.016\n",
      "[44, 500] Loss: 0.019\n",
      "[44, 600] Loss: 0.015\n",
      "[45, 100] Loss: 0.020\n",
      "[45, 200] Loss: 0.018\n",
      "[45, 300] Loss: 0.018\n",
      "[45, 400] Loss: 0.014\n",
      "[45, 500] Loss: 0.018\n",
      "[45, 600] Loss: 0.020\n",
      "[46, 100] Loss: 0.023\n",
      "[46, 200] Loss: 0.016\n",
      "[46, 300] Loss: 0.018\n",
      "[46, 400] Loss: 0.020\n",
      "[46, 500] Loss: 0.017\n",
      "[46, 600] Loss: 0.014\n",
      "[47, 100] Loss: 0.021\n",
      "[47, 200] Loss: 0.012\n",
      "[47, 300] Loss: 0.017\n",
      "[47, 400] Loss: 0.018\n",
      "[47, 500] Loss: 0.017\n",
      "[47, 600] Loss: 0.019\n",
      "[48, 100] Loss: 0.020\n",
      "[48, 200] Loss: 0.015\n",
      "[48, 300] Loss: 0.016\n",
      "[48, 400] Loss: 0.016\n",
      "[48, 500] Loss: 0.014\n",
      "[48, 600] Loss: 0.015\n",
      "[49, 100] Loss: 0.025\n",
      "[49, 200] Loss: 0.011\n",
      "[49, 300] Loss: 0.016\n",
      "[49, 400] Loss: 0.010\n",
      "[49, 500] Loss: 0.017\n",
      "[49, 600] Loss: 0.016\n",
      "[50, 100] Loss: 0.024\n",
      "[50, 200] Loss: 0.011\n",
      "[50, 300] Loss: 0.017\n",
      "[50, 400] Loss: 0.015\n",
      "[50, 500] Loss: 0.015\n",
      "[50, 600] Loss: 0.012\n",
      "[51, 100] Loss: 0.022\n",
      "[51, 200] Loss: 0.012\n",
      "[51, 300] Loss: 0.015\n",
      "[51, 400] Loss: 0.015\n",
      "[51, 500] Loss: 0.011\n",
      "[51, 600] Loss: 0.012\n",
      "[52, 100] Loss: 0.027\n",
      "[52, 200] Loss: 0.014\n",
      "[52, 300] Loss: 0.014\n",
      "[52, 400] Loss: 0.011\n",
      "[52, 500] Loss: 0.013\n",
      "[52, 600] Loss: 0.014\n",
      "[53, 100] Loss: 0.020\n",
      "[53, 200] Loss: 0.013\n",
      "[53, 300] Loss: 0.015\n",
      "[53, 400] Loss: 0.010\n",
      "[53, 500] Loss: 0.011\n",
      "[53, 600] Loss: 0.014\n",
      "[54, 100] Loss: 0.020\n",
      "[54, 200] Loss: 0.012\n",
      "[54, 300] Loss: 0.010\n",
      "[54, 400] Loss: 0.017\n",
      "[54, 500] Loss: 0.011\n",
      "[54, 600] Loss: 0.015\n",
      "[55, 100] Loss: 0.020\n",
      "[55, 200] Loss: 0.010\n",
      "[55, 300] Loss: 0.014\n",
      "[55, 400] Loss: 0.012\n",
      "[55, 500] Loss: 0.017\n",
      "[55, 600] Loss: 0.014\n",
      "[56, 100] Loss: 0.018\n",
      "[56, 200] Loss: 0.012\n",
      "[56, 300] Loss: 0.010\n",
      "[56, 400] Loss: 0.012\n",
      "[56, 500] Loss: 0.014\n",
      "[56, 600] Loss: 0.015\n",
      "[57, 100] Loss: 0.016\n",
      "[57, 200] Loss: 0.012\n",
      "[57, 300] Loss: 0.009\n",
      "[57, 400] Loss: 0.010\n",
      "[57, 500] Loss: 0.014\n",
      "[57, 600] Loss: 0.011\n",
      "[58, 100] Loss: 0.019\n",
      "[58, 200] Loss: 0.009\n",
      "[58, 300] Loss: 0.013\n",
      "[58, 400] Loss: 0.009\n",
      "[58, 500] Loss: 0.011\n",
      "[58, 600] Loss: 0.015\n",
      "[59, 100] Loss: 0.014\n",
      "[59, 200] Loss: 0.009\n",
      "[59, 300] Loss: 0.013\n",
      "[59, 400] Loss: 0.013\n",
      "[59, 500] Loss: 0.010\n",
      "[59, 600] Loss: 0.011\n",
      "[60, 100] Loss: 0.018\n",
      "[60, 200] Loss: 0.011\n",
      "[60, 300] Loss: 0.012\n",
      "[60, 400] Loss: 0.009\n",
      "[60, 500] Loss: 0.017\n",
      "[60, 600] Loss: 0.010\n",
      "[61, 100] Loss: 0.018\n",
      "[61, 200] Loss: 0.012\n",
      "[61, 300] Loss: 0.010\n",
      "[61, 400] Loss: 0.007\n",
      "[61, 500] Loss: 0.012\n",
      "[61, 600] Loss: 0.011\n",
      "[62, 100] Loss: 0.014\n",
      "[62, 200] Loss: 0.010\n",
      "[62, 300] Loss: 0.009\n",
      "[62, 400] Loss: 0.011\n",
      "[62, 500] Loss: 0.011\n",
      "[62, 600] Loss: 0.016\n",
      "[63, 100] Loss: 0.017\n",
      "[63, 200] Loss: 0.010\n",
      "[63, 300] Loss: 0.011\n",
      "[63, 400] Loss: 0.007\n",
      "[63, 500] Loss: 0.010\n",
      "[63, 600] Loss: 0.009\n",
      "[64, 100] Loss: 0.016\n",
      "[64, 200] Loss: 0.010\n",
      "[64, 300] Loss: 0.011\n",
      "[64, 400] Loss: 0.010\n",
      "[64, 500] Loss: 0.011\n",
      "[64, 600] Loss: 0.009\n",
      "[65, 100] Loss: 0.014\n",
      "[65, 200] Loss: 0.012\n",
      "[65, 300] Loss: 0.010\n",
      "[65, 400] Loss: 0.013\n",
      "[65, 500] Loss: 0.008\n",
      "[65, 600] Loss: 0.009\n",
      "[66, 100] Loss: 0.012\n",
      "[66, 200] Loss: 0.009\n",
      "[66, 300] Loss: 0.010\n",
      "[66, 400] Loss: 0.008\n",
      "[66, 500] Loss: 0.010\n",
      "[66, 600] Loss: 0.009\n",
      "[67, 100] Loss: 0.016\n",
      "[67, 200] Loss: 0.010\n",
      "[67, 300] Loss: 0.010\n",
      "[67, 400] Loss: 0.009\n",
      "[67, 500] Loss: 0.008\n",
      "[67, 600] Loss: 0.009\n",
      "[68, 100] Loss: 0.018\n",
      "[68, 200] Loss: 0.007\n",
      "[68, 300] Loss: 0.010\n",
      "[68, 400] Loss: 0.009\n",
      "[68, 500] Loss: 0.010\n",
      "[68, 600] Loss: 0.009\n",
      "[69, 100] Loss: 0.021\n",
      "[69, 200] Loss: 0.009\n",
      "[69, 300] Loss: 0.010\n",
      "[69, 400] Loss: 0.011\n",
      "[69, 500] Loss: 0.012\n",
      "[69, 600] Loss: 0.009\n",
      "[70, 100] Loss: 0.014\n",
      "[70, 200] Loss: 0.007\n",
      "[70, 300] Loss: 0.009\n",
      "[70, 400] Loss: 0.010\n",
      "[70, 500] Loss: 0.008\n",
      "[70, 600] Loss: 0.010\n",
      "[71, 100] Loss: 0.018\n",
      "[71, 200] Loss: 0.009\n",
      "[71, 300] Loss: 0.007\n",
      "[71, 400] Loss: 0.007\n",
      "[71, 500] Loss: 0.006\n",
      "[71, 600] Loss: 0.009\n",
      "[72, 100] Loss: 0.013\n",
      "[72, 200] Loss: 0.007\n",
      "[72, 300] Loss: 0.007\n",
      "[72, 400] Loss: 0.009\n",
      "[72, 500] Loss: 0.010\n",
      "[72, 600] Loss: 0.009\n",
      "[73, 100] Loss: 0.014\n",
      "[73, 200] Loss: 0.008\n",
      "[73, 300] Loss: 0.009\n",
      "[73, 400] Loss: 0.006\n",
      "[73, 500] Loss: 0.009\n",
      "[73, 600] Loss: 0.011\n",
      "[74, 100] Loss: 0.010\n",
      "[74, 200] Loss: 0.008\n",
      "[74, 300] Loss: 0.009\n",
      "[74, 400] Loss: 0.006\n",
      "[74, 500] Loss: 0.008\n",
      "[74, 600] Loss: 0.010\n",
      "[75, 100] Loss: 0.014\n",
      "[75, 200] Loss: 0.008\n",
      "[75, 300] Loss: 0.007\n",
      "[75, 400] Loss: 0.008\n",
      "[75, 500] Loss: 0.008\n",
      "[75, 600] Loss: 0.006\n",
      "[76, 100] Loss: 0.013\n",
      "[76, 200] Loss: 0.008\n",
      "[76, 300] Loss: 0.010\n",
      "[76, 400] Loss: 0.006\n",
      "[76, 500] Loss: 0.010\n",
      "[76, 600] Loss: 0.008\n",
      "[77, 100] Loss: 0.010\n",
      "[77, 200] Loss: 0.007\n",
      "[77, 300] Loss: 0.009\n",
      "[77, 400] Loss: 0.006\n",
      "[77, 500] Loss: 0.008\n",
      "[77, 600] Loss: 0.007\n",
      "[78, 100] Loss: 0.009\n",
      "[78, 200] Loss: 0.008\n",
      "[78, 300] Loss: 0.009\n",
      "[78, 400] Loss: 0.008\n",
      "[78, 500] Loss: 0.007\n",
      "[78, 600] Loss: 0.010\n",
      "[79, 100] Loss: 0.008\n",
      "[79, 200] Loss: 0.006\n",
      "[79, 300] Loss: 0.011\n",
      "[79, 400] Loss: 0.008\n",
      "[79, 500] Loss: 0.007\n",
      "[79, 600] Loss: 0.006\n",
      "[80, 100] Loss: 0.008\n",
      "[80, 200] Loss: 0.006\n",
      "[80, 300] Loss: 0.006\n",
      "[80, 400] Loss: 0.011\n",
      "[80, 500] Loss: 0.012\n",
      "[80, 600] Loss: 0.006\n",
      "[81, 100] Loss: 0.012\n",
      "[81, 200] Loss: 0.007\n",
      "[81, 300] Loss: 0.006\n",
      "[81, 400] Loss: 0.006\n",
      "[81, 500] Loss: 0.007\n",
      "[81, 600] Loss: 0.007\n",
      "[82, 100] Loss: 0.015\n",
      "[82, 200] Loss: 0.008\n",
      "[82, 300] Loss: 0.008\n",
      "[82, 400] Loss: 0.006\n",
      "[82, 500] Loss: 0.008\n",
      "[82, 600] Loss: 0.006\n",
      "[83, 100] Loss: 0.009\n",
      "[83, 200] Loss: 0.007\n",
      "[83, 300] Loss: 0.006\n",
      "[83, 400] Loss: 0.005\n",
      "[83, 500] Loss: 0.005\n",
      "[83, 600] Loss: 0.010\n",
      "[84, 100] Loss: 0.010\n",
      "[84, 200] Loss: 0.007\n",
      "[84, 300] Loss: 0.007\n",
      "[84, 400] Loss: 0.009\n",
      "[84, 500] Loss: 0.005\n",
      "[84, 600] Loss: 0.008\n",
      "[85, 100] Loss: 0.008\n",
      "[85, 200] Loss: 0.005\n",
      "[85, 300] Loss: 0.007\n",
      "[85, 400] Loss: 0.009\n",
      "[85, 500] Loss: 0.006\n",
      "[85, 600] Loss: 0.008\n",
      "[86, 100] Loss: 0.008\n",
      "[86, 200] Loss: 0.004\n",
      "[86, 300] Loss: 0.006\n",
      "[86, 400] Loss: 0.007\n",
      "[86, 500] Loss: 0.006\n",
      "[86, 600] Loss: 0.006\n",
      "[87, 100] Loss: 0.011\n",
      "[87, 200] Loss: 0.005\n",
      "[87, 300] Loss: 0.006\n",
      "[87, 400] Loss: 0.004\n",
      "[87, 500] Loss: 0.008\n",
      "[87, 600] Loss: 0.004\n",
      "[88, 100] Loss: 0.012\n",
      "[88, 200] Loss: 0.004\n",
      "[88, 300] Loss: 0.006\n",
      "[88, 400] Loss: 0.007\n",
      "[88, 500] Loss: 0.008\n",
      "[88, 600] Loss: 0.007\n",
      "[89, 100] Loss: 0.010\n",
      "[89, 200] Loss: 0.007\n",
      "[89, 300] Loss: 0.007\n",
      "[89, 400] Loss: 0.007\n",
      "[89, 500] Loss: 0.005\n",
      "[89, 600] Loss: 0.008\n",
      "[90, 100] Loss: 0.010\n",
      "[90, 200] Loss: 0.007\n",
      "[90, 300] Loss: 0.006\n",
      "[90, 400] Loss: 0.005\n",
      "[90, 500] Loss: 0.006\n",
      "[90, 600] Loss: 0.005\n",
      "[91, 100] Loss: 0.009\n",
      "[91, 200] Loss: 0.007\n",
      "[91, 300] Loss: 0.006\n",
      "[91, 400] Loss: 0.008\n",
      "[91, 500] Loss: 0.008\n",
      "[91, 600] Loss: 0.006\n",
      "[92, 100] Loss: 0.012\n",
      "[92, 200] Loss: 0.006\n",
      "[92, 300] Loss: 0.006\n",
      "[92, 400] Loss: 0.006\n",
      "[92, 500] Loss: 0.006\n",
      "[92, 600] Loss: 0.007\n",
      "[93, 100] Loss: 0.008\n",
      "[93, 200] Loss: 0.007\n",
      "[93, 300] Loss: 0.006\n",
      "[93, 400] Loss: 0.006\n",
      "[93, 500] Loss: 0.006\n",
      "[93, 600] Loss: 0.007\n",
      "[94, 100] Loss: 0.008\n",
      "[94, 200] Loss: 0.003\n",
      "[94, 300] Loss: 0.006\n",
      "[94, 400] Loss: 0.005\n",
      "[94, 500] Loss: 0.006\n",
      "[94, 600] Loss: 0.006\n",
      "[95, 100] Loss: 0.009\n",
      "[95, 200] Loss: 0.006\n",
      "[95, 300] Loss: 0.005\n",
      "[95, 400] Loss: 0.006\n",
      "[95, 500] Loss: 0.005\n",
      "[95, 600] Loss: 0.007\n",
      "[96, 100] Loss: 0.009\n",
      "[96, 200] Loss: 0.003\n",
      "[96, 300] Loss: 0.005\n",
      "[96, 400] Loss: 0.007\n",
      "[96, 500] Loss: 0.007\n",
      "[96, 600] Loss: 0.005\n",
      "[97, 100] Loss: 0.009\n",
      "[97, 200] Loss: 0.005\n",
      "[97, 300] Loss: 0.005\n",
      "[97, 400] Loss: 0.007\n",
      "[97, 500] Loss: 0.005\n",
      "[97, 600] Loss: 0.005\n",
      "[98, 100] Loss: 0.007\n",
      "[98, 200] Loss: 0.004\n",
      "[98, 300] Loss: 0.003\n",
      "[98, 400] Loss: 0.003\n",
      "[98, 500] Loss: 0.009\n",
      "[98, 600] Loss: 0.006\n",
      "[99, 100] Loss: 0.008\n",
      "[99, 200] Loss: 0.005\n",
      "[99, 300] Loss: 0.004\n",
      "[99, 400] Loss: 0.006\n",
      "[99, 500] Loss: 0.005\n",
      "[99, 600] Loss: 0.007\n",
      "[100, 100] Loss: 0.006\n",
      "[100, 200] Loss: 0.007\n",
      "[100, 300] Loss: 0.006\n",
      "[100, 400] Loss: 0.006\n",
      "[100, 500] Loss: 0.005\n",
      "[100, 600] Loss: 0.005\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "running_loss = 0.0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            print(f'[{epoch + 1}, {i + 1}] Loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "            \n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23c06552",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        outputs = model(data)\n",
    "\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "        predictions.extend(pred.tolist())\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ImageId' : range(1, len(predictions) + 1),\n",
    "    'Label': predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('./../submissions/digit-recognizer-submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
